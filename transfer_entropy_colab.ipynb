{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3c0bfb",
   "metadata": {
    "id": "1d3c0bfb"
   },
   "source": [
    "# Transfer Entropy (Standalone, Colab-Ready)\n",
    "\n",
    "- Standalone pipeline based on IDTxl and (spectral multivariate transfer entropy by Edoardo Pinzuti):\n",
    "- Fast delay scan (Gaussian-TE/GC) to select optimal source→target lag per link.\n",
    "- Band-wise TE on filter bank: delta, theta, alpha, beta, gamma, broadband.\n",
    "- Phase-randomized surrogates for source-only and target-only p-values; FDR-corrected.\n",
    "- Heatmaps (TE and significance) and an optional \"winning band\" map.\n",
    "- Batch mode: analyze one or many EDFs; match annotation TXT by EDF prefix (before first underscore).\n",
    "- Per-EDF subfolder: saves delay scan results, figures, winning band map, and a text summary.\n",
    "\n",
    "## References\n",
    "- IDTxl GitHub: https://github.com/pwollstadt/IDTxl\n",
    "- Spectral TE branch: https://github.com/pwollstadt/IDTxl/tree/feature_spectral_te/\n",
    "- Wollstadt, P., Lizier, J. T., Vicente, R., Finn, C., Martinez-Zarzuela, M., Mediano, P., Novelli, L., & Wibral, M. (2018). IDTxl: The Information Dynamics Toolkit xl: a Python package for the efficient analysis of multivariate information dynamics in networks. Journal of Open Source Software, 4(34), 1081. https://doi.org/10.21105/joss.01081"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7a0f0",
   "metadata": {},
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8iqLZdkYxtis",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1808,
     "status": "ok",
     "timestamp": 1757196355899,
     "user": {
      "displayName": "Francois",
      "userId": "00618482177956197410"
     },
     "user_tz": -60
    },
    "id": "8iqLZdkYxtis",
    "outputId": "115187e9-b350-4b7b-e337-4ac5ccce3ef7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0e95f",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe0fe6",
   "metadata": {
    "executionInfo": {
     "elapsed": 2472,
     "status": "ok",
     "timestamp": 1757196358369,
     "user": {
      "displayName": "Francois",
      "userId": "00618482177956197410"
     },
     "user_tz": -60
    },
    "id": "a5fe0fe6"
   },
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip -q install mne numpy scipy matplotlib seaborn joblib numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33ed03",
   "metadata": {},
   "source": [
    "## Imports and Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85500bb8",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1757196358397,
     "user": {
      "displayName": "Francois",
      "userId": "00618482177956197410"
     },
     "user_tz": -60
    },
    "id": "85500bb8"
   },
   "outputs": [],
   "source": [
    "# Imports and global config\n",
    "import os, re, json, math, itertools, warnings\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from typing import List, Tuple, Dict\n",
    "warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    import mne\n",
    "except Exception as e:\n",
    "    print('mne not available; please install in Colab cell above')\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "plt.rcParams.update({'figure.dpi': 120})\n",
    "\n",
    "# Default bands (Hz)\n",
    "DEFAULT_BANDS = {\n",
    "    'delta': (0.5, 4.0),\n",
    "    'theta': (4.0, 8.0),\n",
    "    'alpha': (8.0, 13.0),\n",
    "    'beta': (13.0, 30.0),\n",
    "    'gamma': (30.0, 80.0),\n",
    "    'broadband': (0.5, None),  # None => up to Nyquist\n",
    "}\n",
    "\n",
    "# Runtime config\n",
    "N_JOBS = max(1, os.cpu_count() - 1)\n",
    "MAX_DELAY_MS = 200  # delay scan up to 200 ms\n",
    "EMBED_PAST = 1     # past embedding for Gaussian TE\n",
    "N_SURR = 100        # surrogates per test (source-only and target-only)\n",
    "ALPHA = 0.05        # significance level\n",
    "DOWNSAMPLE_BAND = True  # decimate to 2x band high-cut (when possible)\n",
    "# Limit analysis duration to first N seconds to avoid OOM on large recordings\n",
    "MAX_DURATION_S = 300  # 300 seconds = 5 minutes (set to None to disable)\n",
    "\n",
    "# Plot style\n",
    "CMAP_TE = 'magma'\n",
    "CMAP_BAND = 'tab10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebcb422",
   "metadata": {},
   "source": [
    "## Helper Functions (Filtering, Surrogates, Gaussian TE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fede9",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1757196358414,
     "user": {
      "displayName": "Francois",
      "userId": "00618482177956197410"
     },
     "user_tz": -60
    },
    "id": "b95fede9"
   },
   "outputs": [],
   "source": [
    "# Helpers: filtering, surrogates, Gaussian TE (equiv. to GC under Gaussianity)\n",
    "def get_sos_band(fs: float, low: float, high: float, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    if high is None:\n",
    "        # high is Nyquist minus margin\n",
    "        high = 0.99 * nyq\n",
    "    low = max(0.001, low)\n",
    "    high = min(high, nyq * 0.99)\n",
    "    if low <= 0 and high < nyq:\n",
    "        wn = high / nyq\n",
    "        btype = 'lowpass'\n",
    "        sos = signal.butter(order, wn, btype=btype, output='sos')\n",
    "    elif low > 0 and (high >= nyq * 0.99):\n",
    "        wn = low / nyq\n",
    "        btype = 'highpass'\n",
    "        sos = signal.butter(order, wn, btype=btype, output='sos')\n",
    "    else:\n",
    "        wn = [low / nyq, high / nyq]\n",
    "        sos = signal.butter(order, wn, btype='bandpass', output='sos')\n",
    "    return sos\n",
    "\n",
    "def bandpass(ts: np.ndarray, fs: float, band: Tuple[float, float], order=4, decimate=True):\n",
    "    ts = np.asarray(ts, float)\n",
    "    sos = get_sos_band(fs, band[0], band[1], order=order)\n",
    "    y = signal.sosfiltfilt(sos, ts, axis=0)\n",
    "    ds = 1\n",
    "    if decimate and band[1] is not None and band[1] > 0:\n",
    "        target_fs = min(fs, max(2.5 * band[1], 2 * band[1] + 1))\n",
    "        ds = max(1, int(math.floor(fs / target_fs)))\n",
    "    if ds > 1:\n",
    "        y = signal.decimate(y, ds, axis=0, ftype='fir', zero_phase=True)\n",
    "        fs_eff = fs / ds\n",
    "    else:\n",
    "        fs_eff = fs\n",
    "    return y, fs_eff\n",
    "\n",
    "def phase_randomize(ts: np.ndarray, rng: np.random.Generator):\n",
    "    x = np.asarray(ts, float)\n",
    "    n = len(x)\n",
    "    Xf = np.fft.rfft(x)\n",
    "    mags = np.abs(Xf)\n",
    "    phases = np.angle(Xf)\n",
    "    rand_ph = rng.uniform(-np.pi, np.pi, size=phases.shape)\n",
    "    rand_ph[0] = 0.0\n",
    "    if n % 2 == 0:\n",
    "        rand_ph[-1] = 0.0\n",
    "    Yf = mags * np.exp(1j * (phases + rand_ph))\n",
    "    y = np.fft.irfft(Yf, n=n)\n",
    "    return y\n",
    "\n",
    "def build_reg_mats(xpast: np.ndarray, ypast: np.ndarray, yt: np.ndarray):\n",
    "    # Assemble design matrices for Gaussian TE\n",
    "    # Model 1: yt ~ ypast\n",
    "    X1 = ypast\n",
    "    # Model 2: yt ~ ypast + xpast\n",
    "    X2 = np.hstack([ypast, xpast])\n",
    "    return X1, X2, yt\n",
    "\n",
    "def ls_res_var(X: np.ndarray, y: np.ndarray):\n",
    "    beta, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "    resid = y - X @ beta\n",
    "    return float(np.var(resid, ddof=min(X.shape[1], X.shape[0]) - 1 if X.shape[0] > X.shape[1] else 1))\n",
    "\n",
    "def gaussian_te(x: np.ndarray, y: np.ndarray, delay_samples: int, k_past=1):\n",
    "    # TE_{x->y} with Gaussian assumption: 0.5 * ln(var(e1)/var(e2))\n",
    "    # Build past vectors\n",
    "    d = int(delay_samples)\n",
    "    if d <= 0:\n",
    "        return np.nan\n",
    "    # Align so Xpast leads Yt by d\n",
    "    T = min(len(x) - d - k_past, len(y) - k_past)\n",
    "    if T <= 5:\n",
    "        return np.nan\n",
    "    idx_y = np.arange(k_past, k_past + T)\n",
    "    idx_x = np.arange(k_past + d, k_past + d + T)\n",
    "    Yt = y[idx_y]  # current\n",
    "    Ypast = np.column_stack([y[idx_y - i] for i in range(1, k_past + 1)])\n",
    "    Xpast = np.column_stack([x[idx_x - i] for i in range(1, k_past + 1)])\n",
    "    X1, X2, Y = build_reg_mats(Xpast, Ypast, Yt)\n",
    "    v1 = ls_res_var(X1, Y)\n",
    "    v2 = ls_res_var(X2, Y)\n",
    "    if v1 <= 0 or v2 <= 0:\n",
    "        return np.nan\n",
    "    return 0.5 * np.log(v1 / v2)\n",
    "\n",
    "def fdr_bh(pvals: np.ndarray, alpha=0.05):\n",
    "    p = np.asarray(pvals).flatten()\n",
    "    n = p.size\n",
    "    idx = np.argsort(p)\n",
    "    ranked = p[idx]\n",
    "    thresh = alpha * (np.arange(1, n + 1) / n)\n",
    "    passed = ranked <= thresh\n",
    "    k = np.where(passed)[0].max() + 1 if np.any(passed) else 0\n",
    "    crit = ranked[k - 1] if k > 0 else 0.0\n",
    "    return (pvals <= crit), crit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e637da2",
   "metadata": {},
   "source": [
    "## EDF/FIF Loader and Batch Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d3844",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1757196358519,
     "user": {
      "displayName": "Francois",
      "userId": "00618482177956197410"
     },
     "user_tz": -60
    },
    "id": "a78d3844",
    "outputId": "86d11cee-3f4c-4956-b259-56a44fca561b"
   },
   "outputs": [],
   "source": [
    "# EDF/FIF + annotations + batch runner\n",
    "import csv\n",
    "\n",
    "\n",
    "def find_annotations_for_edf_like(path: str, ann_folder: str):\n",
    "    base = os.path.basename(path)\n",
    "    prefix = base.split('_')[0]\n",
    "    for fn in os.listdir(ann_folder):\n",
    "        if fn.lower().endswith('.txt') and fn.startswith(prefix):\n",
    "            return os.path.join(ann_folder, fn)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def load_signal_any(path: str, max_duration_s=None):\n",
    "    \"\"\"Load .edf or .fif and optionally crop to first max_duration_s seconds to limit memory use.\n",
    "    Returns: ch_names, data (n_channels x n_samples), fs\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == '.edf':\n",
    "        raw = mne.io.read_raw_edf(path, preload=True, verbose='ERROR')\n",
    "    elif ext == '.fif':\n",
    "        raw = mne.io.read_raw_fif(path, preload=True, verbose='ERROR')\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported file type: {ext}. Expected .edf or .fif')\n",
    "\n",
    "    # Optionally crop to limit duration (helps avoid GPU/CPU OOM)\n",
    "    try:\n",
    "        if max_duration_s is not None:\n",
    "            # crop modifies Raw in-place; keep from 0 to max_duration_s\n",
    "            raw.crop(tmin=0.0, tmax=float(max_duration_s))\n",
    "    except Exception:\n",
    "        # If cropping fails, continue with full data\n",
    "        pass\n",
    "\n",
    "    # Attempt unit normalization to Volts (MNE stores SI units typically already in Volts)\n",
    "    try:\n",
    "        units = raw.get_units()\n",
    "        # If microvolts were used in EDF, ensure conversion to Volts\n",
    "        for ch, u in units.items():\n",
    "            if isinstance(u, str) and u.lower() in ('uv', 'µv', 'microvolts', 'microvolt'):\n",
    "                raw.apply_function(lambda x: x * 1e-6)\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "    data = raw.get_data()  # n_channels x n_samples\n",
    "    fs = float(raw.info['sfreq'])\n",
    "    return raw.ch_names, data, fs\n",
    "\n",
    "\n",
    "# Refined override: parse SOZ/NIZ annotations (3-column) with de-dup and diagnostics\n",
    "# Format per line: <region>\\t<channel_name>\\t<label>\n",
    "# label 1 => SOZ (targets), 0 => NIZ (sources)\n",
    "\n",
    "def parse_links_from_txt(txt_path: str, ch_names: List[str]) -> List[Tuple[int, int]]:\n",
    "    links: List[Tuple[int, int]] = []\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "    idx = {n: i for i, n in enumerate(ch_names)}\n",
    "\n",
    "    # 1) sources:/targets: format\n",
    "    srcs = None\n",
    "    tgts = None\n",
    "    for ln in lines:\n",
    "        if ln.lower().startswith('sources:'):\n",
    "            srcs = [s.strip() for s in re.split('[,;]+', ln.split(':', 1)[1]) if s.strip()]\n",
    "        elif ln.lower().startswith('targets:'):\n",
    "            tgts = [s.strip() for s in re.split('[,;]+', ln.split(':', 1)[1]) if s.strip()]\n",
    "    if srcs is not None and tgts is not None:\n",
    "        for s in srcs:\n",
    "            for t in tgts:\n",
    "                if s in idx and t in idx and idx[s] != idx[t]:\n",
    "                    links.append((idx[s], idx[t]))\n",
    "        if links:\n",
    "            print(f\"Parsed {len(links)} links from sources:/targets: format\")\n",
    "            return links\n",
    "\n",
    "    # 2) SOZ/NIZ 3-column format\n",
    "    labeled = {}  # channel -> label (last wins if duplicated)\n",
    "    for ln in lines:\n",
    "        parts = re.split(r'[\\t, ]+', ln)\n",
    "        if len(parts) >= 3:\n",
    "            chan = parts[1].strip()\n",
    "            try:\n",
    "                lab = float(parts[2])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if chan in idx:\n",
    "                labeled[chan] = lab\n",
    "    if labeled:\n",
    "        src_list = sorted({c for c, l in labeled.items() if l <= 0.5})  # NIZ\n",
    "        tgt_list = sorted({c for c, l in labeled.items() if l > 0.5})   # SOZ\n",
    "        pair_set = set()\n",
    "        for s in src_list:\n",
    "            for t in tgt_list:\n",
    "                if s in idx and t in idx and idx[s] != idx[t]:\n",
    "                    pair_set.add((idx[s], idx[t]))\n",
    "        links = sorted(pair_set)\n",
    "        print(f\"Parsed {len(links)} links from SOZ/NIZ format ({len(src_list)} sources x {len(tgt_list)} targets)\")\n",
    "        missing = [c for c in list(labeled.keys()) if c not in idx]\n",
    "        if missing:\n",
    "            print(f\"Warning: {len(missing)} channels not found in recording and were ignored: {missing[:10]}{'...' if len(missing)>10 else ''}\")\n",
    "        return links\n",
    "\n",
    "    # 3) Fallback: pair-per-line\n",
    "    pair_set = set()\n",
    "    for ln in lines:\n",
    "        parts = re.split(r'[\\t, ]+', ln)\n",
    "        if len(parts) >= 2 and parts[0] in idx and parts[1] in idx and parts[0] != parts[1]:\n",
    "            pair_set.add((idx[parts[0]], idx[parts[1]]))\n",
    "    links = sorted(pair_set)\n",
    "    if links:\n",
    "        print(f\"Parsed {len(links)} links from pair-per-line format\")\n",
    "    return links\n",
    "\n",
    "print('Annotation parser override (SOZ/NIZ) active.')\n",
    "\n",
    "\n",
    "def save_delay_results(path, links, delays, te_traces, fs):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('# Delay scan results\\n')\n",
    "        f.write('# format: src\\tgt\\tbest_delay_samples\\tbest_delay_ms\\n')\n",
    "        for (s, t), d, trace in zip(links, delays, te_traces):\n",
    "            f.write(f\"{s}\\t{t}\\t{d}\\t{1000.0 * d / fs:.3f}\\n\")\n",
    "\n",
    "\n",
    "def save_summary_txt(path, names, TE, pS, pT, sigS, sigT):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('Bands: ' + ','.join(names) + '\\n')\n",
    "        f.write('TE matrix (rows=src, cols=tgt):\\n')\n",
    "        np.savetxt(f, TE, fmt='%.6f')\n",
    "        f.write('\\nP_source:\\n'); np.savetxt(f, pS, fmt='%.6f')\n",
    "        f.write('\\nP_target:\\n'); np.savetxt(f, pT, fmt='%.6f')\n",
    "        f.write('\\nSig_source (FDR):\\n'); np.savetxt(f, sigS.astype(int), fmt='%d')\n",
    "        f.write('\\nSig_target (FDR):\\n'); np.savetxt(f, sigT.astype(int), fmt='%d')\n",
    "\n",
    "\n",
    "def process_one_file(path: str, ann_folder: str, bands=DEFAULT_BANDS, out_root=None):\n",
    "    \"\"\"\n",
    "    Process one EDF/FIF file and produce per-link outputs and one aggregated file-level summary.\n",
    "    \"\"\"\n",
    "    # Load only up to MAX_DURATION_S seconds to limit memory/GPU usage\n",
    "    ch_names, data, fs = load_signal_any(path, max_duration_s=MAX_DURATION_S)\n",
    "    ann = find_annotations_for_edf_like(path, ann_folder)\n",
    "    if ann is None:\n",
    "        raise FileNotFoundError(f'No annotation TXT matching prefix for {path}')\n",
    "    links = parse_links_from_txt(ann, ch_names)\n",
    "    if not links:\n",
    "        raise ValueError(f'No valid (src,tgt) links parsed from {ann}')\n",
    "\n",
    "    base = os.path.basename(path)\n",
    "    prefix = base.split('_')[0]\n",
    "    out_dir = os.path.join(os.path.dirname(path) if out_root is None else out_root, prefix)\n",
    "    # Ensure the output directory exists before saving files\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Delay scan per link (broadband)\n",
    "    bb_lo, bb_hi = bands['broadband']\n",
    "    x_bb = {}\n",
    "    for idx in set([s for s, _ in links] + [t for _, t in links]):\n",
    "        x = data[idx] - np.mean(data[idx])\n",
    "        xf, _ = bandpass(x, fs, (bb_lo, bb_hi), order=4, decimate=False)\n",
    "        x_bb[idx] = xf\n",
    "    delays = []\n",
    "    te_traces = []\n",
    "    for (s, t) in links:\n",
    "        d, trace = delay_scan(x_bb[s], x_bb[t], fs, max_delay_ms=MAX_DELAY_MS, k_past=EMBED_PAST)\n",
    "        delays.append(int(d))\n",
    "        te_traces.append(trace)\n",
    "    save_delay_results(os.path.join(out_dir, 'delay_scan_results.txt'), links, delays, te_traces, fs)\n",
    "\n",
    "    # Band-wise TE per link + plots; collect results for aggregation\n",
    "    aggregated_signifs = []\n",
    "    for (s, t), d in zip(links, delays):\n",
    "        names, TE, pS, pT, sigS, sigT, critS, critT = compute_te_grid_with_surrogates(\n",
    "            data[s] - np.mean(data[s]), data[t] - np.mean(data[t]), fs, bands, delay_samples=d,\n",
    "            k_past=EMBED_PAST, n_surr=N_SURR, alpha=ALPHA, downsample=DOWNSAMPLE_BAND, rng=np.random.default_rng(123))\n",
    "        base_out = os.path.join(out_dir, f'link_{s}_{t}')\n",
    "        plot_te_heatmap(names, TE, pS, pT, base_out)\n",
    "        wins = plot_winning_band_map(names, TE, pS, pT, ALPHA, base_out + '_winning_band.png')\n",
    "        np.savez_compressed(base_out + '.npz', names=np.array(names), TE=TE, p_source=pS, p_target=pT, sig_source=sigS, sig_target=sigT, delay_samples=int(d), fs=float(fs), wins=wins)\n",
    "        save_summary_txt(base_out + '.txt', names, TE, pS, pT, sigS, sigT)\n",
    "        aggregated_signifs.append({'pS': pS, 'pT': pT, 'names': names})\n",
    "\n",
    "    # Aggregate across links to form file-level summary\n",
    "    if not aggregated_signifs:\n",
    "        print('No link results to aggregate.')\n",
    "        return out_dir\n",
    "\n",
    "    band_names = aggregated_signifs[0]['names']\n",
    "    B = len(band_names)\n",
    "    total_src_counts = np.zeros(B, dtype=int)\n",
    "    total_tgt_counts = np.zeros(B, dtype=int)\n",
    "    for res in aggregated_signifs:\n",
    "        signif = (res['pS'] <= ALPHA) & (res['pT'] <= ALPHA)\n",
    "        total_src_counts += np.sum(signif, axis=1)\n",
    "        total_tgt_counts += np.sum(signif, axis=0)\n",
    "\n",
    "    # Save aggregated counts CSV (use csv writer to avoid extra deps)\n",
    "    base_out_file = os.path.join(out_dir, prefix)\n",
    "    with open(base_out_file + '_band_counts.csv', 'w', newline='') as cf:\n",
    "        writer = csv.writer(cf)\n",
    "        writer.writerow(['band', 'total_src_count', 'total_tgt_count'])\n",
    "        for nm, s, t in zip(band_names, total_src_counts, total_tgt_counts):\n",
    "            writer.writerow([nm, int(s), int(t)])\n",
    "\n",
    "    # Save nested pie summary\n",
    "    plot_summary_pie(band_names, total_src_counts, total_tgt_counts, base_out_file + '_summary_pie.png')\n",
    "\n",
    "    # Save text summary\n",
    "    with open(base_out_file + '_summary.txt', 'w') as fh:\n",
    "        fh.write(f'# Aggregated significant band counts for {os.path.basename(path)}\\n')\n",
    "        fh.write('# format: band, total_source_count, total_target_count\\n')\n",
    "        for nm, s, t in zip(band_names, total_src_counts, total_tgt_counts):\n",
    "            fh.write(f\"{nm},{s},{t}\\n\")\n",
    "\n",
    "    print('Saved per-link results and file-wide summary to:', out_dir)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def run_batch():\n",
    "    mode = input('Analyze (1) one file or (2) multiple files in a folder? Enter 1 or 2: ').strip()\n",
    "    if mode == '1':\n",
    "        path = input('Enter full path to the EDF/FIF: ').strip()\n",
    "        if not os.path.isfile(path):\n",
    "            raise FileNotFoundError('File not found')\n",
    "        ann_folder = input('Enter folder path containing annotation TXTs: ').strip()\n",
    "        if not os.path.isdir(ann_folder):\n",
    "            raise FileNotFoundError('Annotation folder not found')\n",
    "        process_one_file(path, ann_folder)\n",
    "    else:\n",
    "        folder = input('Enter folder path containing EDF/FIF: ').strip()\n",
    "        if not os.path.isdir(folder):\n",
    "            raise FileNotFoundError('Folder not found')\n",
    "        ann_folder = input('Enter folder path containing annotation TXTs: ').strip()\n",
    "        if not os.path.isdir(ann_folder):\n",
    "            raise FileNotFoundError('Annotation folder not found')\n",
    "        files = [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(('.edf', '.fif'))]\n",
    "        for p in sorted(files):\n",
    "            try:\n",
    "                process_one_file(p, ann_folder, out_root=folder)\n",
    "            except Exception as e:\n",
    "                print('Failed for', p, '->', e)\n",
    "\n",
    "print('Ready. Run run_batch() to start interactive processing for EDF/FIF.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81de51",
   "metadata": {},
   "source": [
    "## Time Delay Scaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PG6mwTAxBdqL",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1757196358523,
     "user": {
      "displayName": "Francois",
      "userId": "00618482177956197410"
     },
     "user_tz": -60
    },
    "id": "PG6mwTAxBdqL"
   },
   "outputs": [],
   "source": [
    "# Delay scan helper: TE_{x->y} over delays to select best lag (coarse by default)\n",
    "import numpy as np\n",
    "\n",
    "def delay_scan(x, y, fs, max_delay_ms=200, k_past=1, step_ms=None, n_steps=10, min_delay_ms=1, detrend=True, zscore=True):\n",
    "    \"\"\"\n",
    "    Scan TE_{x->y} over positive delays and pick the best delay (in samples).\n",
    "    Efficient by default via coarse sampling (n_steps) across [min_delay, max_delay].\n",
    "\n",
    "    Inputs:\n",
    "      x, y: 1D arrays\n",
    "      fs: sampling rate (Hz)\n",
    "      max_delay_ms: maximum delay to test (ms)\n",
    "      k_past: past embedding for gaussian_te\n",
    "      step_ms: fixed step in ms (if provided, overrides n_steps)\n",
    "      n_steps: number of evenly-spaced delays to test (default 10)\n",
    "      min_delay_ms: minimum delay (ms)\n",
    "      detrend/zscore: simple preprocessing\n",
    "    Returns:\n",
    "      best_delay_samples (int), te_trace (np.ndarray) aligned to the tested delays (internal)\n",
    "    Notes:\n",
    "      - Uses gaussian_te(x, y, delay_samples, k_past) defined earlier (optionally JIT-accelerated if you ran the GPU/Parallel cell).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    n = min(x.size, y.size)\n",
    "    if n < 10:\n",
    "        return 1, np.array([np.nan])\n",
    "    x = x[:n]; y = y[:n]\n",
    "    if detrend:\n",
    "        x = x - np.nanmean(x); y = y - np.nanmean(y)\n",
    "    if zscore:\n",
    "        sx = np.nanstd(x); sy = np.nanstd(y)\n",
    "        if sx > 0: x = x / sx\n",
    "        if sy > 0: y = y / sy\n",
    "\n",
    "    d_min = max(1, int(round(min_delay_ms * fs / 1000.0)))\n",
    "    d_max = max(d_min, int(round(max_delay_ms * fs / 1000.0)))\n",
    "\n",
    "    if step_ms is not None:\n",
    "        step_samp = max(1, int(round(step_ms * fs / 1000.0)))\n",
    "        delays = np.arange(d_min, d_max + 1, step_samp, dtype=int)\n",
    "    else:\n",
    "        # Evenly-spaced integer sample delays\n",
    "        n_steps = max(1, int(n_steps))\n",
    "        delays = np.unique(np.linspace(d_min, d_max, num=n_steps, dtype=int))\n",
    "        if delays.size == 0:\n",
    "            delays = np.array([d_min], dtype=int)\n",
    "\n",
    "    te_vals = np.full(delays.shape[0], np.nan, dtype=float)\n",
    "\n",
    "    for i, d in enumerate(delays):\n",
    "        try:\n",
    "            te_vals[i] = gaussian_te(x, y, int(d), k_past=k_past)\n",
    "        except Exception:\n",
    "            te_vals[i] = np.nan\n",
    "\n",
    "    if not np.any(np.isfinite(te_vals)):\n",
    "        return int(delays[0]), te_vals\n",
    "    best_idx = int(np.nanargmax(te_vals))\n",
    "    best_delay = int(delays[best_idx])\n",
    "    return best_delay, te_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f95b3",
   "metadata": {},
   "source": [
    "## Visualization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d83a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1757196358526,
     "user": {
      "displayName": "Francois",
      "userId": "00618482177956197410"
     },
     "user_tz": -60
    },
    "id": "490d83a3"
   },
   "outputs": [],
   "source": [
    "# Visualization helpers\n",
    "def plot_te_heatmap(names, TE, pS, pT, out_png_base):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(TE, xticklabels=names, yticklabels=names, cmap=CMAP_TE, annot=False)\n",
    "    ax.set_xlabel('Target band')\n",
    "    ax.set_ylabel('Source band')\n",
    "    ax.set_title('Cross-band TE (Gaussian)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png_base + '_TE.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    # p-value heatmaps\n",
    "    for lab, P in [('p_source', pS), ('p_target', pT)]:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = sns.heatmap(P, xticklabels=names, yticklabels=names, cmap='viridis_r', vmin=0, vmax=1)\n",
    "        ax.set_xlabel('Target band')\n",
    "        ax.set_ylabel('Source band')\n",
    "        ax.set_title(f'{lab} (fraction of surrogates ≥ obs)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_png_base + f'_{lab}.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_winning_band_map(names, TE, pS, pT, alpha, out_png):\n",
    "    \"\"\"Draw a vertical column where each row = source band and color = winning target band.\n",
    "    Legend shows gray = no significant winner and a colored swatch for each target band (by name).\n",
    "    \"\"\"\n",
    "    TE = np.asarray(TE)\n",
    "    pS = np.asarray(pS); pT = np.asarray(pT)\n",
    "    signif = (pS <= alpha) & (pT <= alpha)\n",
    "    # significant-only TE, with NaN where not significant\n",
    "    sig_TE = np.where(signif, TE, np.nan)\n",
    "    B = len(names)\n",
    "    # Prepare wins array: -1 means no significant winning band for that source\n",
    "    wins = np.full(B, -1, dtype=int)\n",
    "    # Find rows that have at least one finite value\n",
    "    has_finite = np.any(np.isfinite(sig_TE), axis=1)\n",
    "    for i in np.where(has_finite)[0]:\n",
    "        # np.nanargmax is safe now because row has at least one finite entry\n",
    "        wins[i] = int(np.nanargmax(sig_TE[i, :]))\n",
    "\n",
    "    # color by target band index; index 0 reserved for 'no winner' (gray)\n",
    "    colors = plt.get_cmap(CMAP_BAND)(np.linspace(0, 1, B))\n",
    "    cmap_list = [(0.7, 0.7, 0.7, 1.0)] + [tuple(c) for c in colors]  # 0=none, 1..B bands\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    cmap = ListedColormap(cmap_list)\n",
    "    img = (wins + 1).reshape(-1, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    im = ax.imshow(img, aspect='auto', cmap=cmap, vmin=0, vmax=B)\n",
    "    ax.set_yticks(range(B))\n",
    "    ax.set_yticklabels(names)\n",
    "    ax.set_xticks([0])\n",
    "    ax.set_xticklabels(['win tgt'])\n",
    "    ax.set_title('Winning band (significant-only argmax)')\n",
    "\n",
    "    # Annotation text clarifying mapping\n",
    "    ax.text(1.05, 0.5, 'Rows = source bands\\nColor = winning target band\\nGray = no significant target', transform=ax.transAxes,\n",
    "            fontsize=8, va='center', ha='left', bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "    # Build legend patches: first gray = none, then each target band color\n",
    "    patches = [Patch(facecolor=cmap_list[0], edgecolor='black', label='no significant target (gray)')]\n",
    "    for j, nm in enumerate(names):\n",
    "        # map target j -> cmap_list[j+1]\n",
    "        patches.append(Patch(facecolor=cmap_list[j + 1], edgecolor='black', label=f'target: {nm}'))\n",
    "\n",
    "    # Place legend to the right\n",
    "    ax.legend(handles=patches, bbox_to_anchor=(1.1, 1.0), loc='upper left', fontsize=8, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Return wins for possible saving/inspection\n",
    "    return wins\n",
    "\n",
    "\n",
    "def plot_summary_pie(names, src_counts, tgt_counts, out_png):\n",
    "    \"\"\"Nested pie: outer ring = source counts (lighter shades), inner ring = target counts (darker shades).\n",
    "    Both rings use the same base color per band; outer slices are lighter to indicate source.\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    from matplotlib.patches import Patch as _Patch\n",
    "\n",
    "    names = list(names)\n",
    "    B = len(names)\n",
    "    src = _np.asarray(src_counts, dtype=float)\n",
    "    tgt = _np.asarray(tgt_counts, dtype=float)\n",
    "\n",
    "    # If no significant pairs, save a simple informational figure\n",
    "    if src.sum() == 0 and tgt.sum() == 0:\n",
    "        fig, ax = plt.subplots(figsize=(5, 4))\n",
    "        ax.text(0.5, 0.5, 'No significant source-target pairs found', ha='center', va='center')\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_png, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return\n",
    "\n",
    "    base_colors = plt.get_cmap(CMAP_BAND)(_np.linspace(0, 1, B))\n",
    "    # lighter for source (blend with white), darker for target\n",
    "    light_colors = [tuple(0.65 * c[:3] + 0.35 * _np.array([1.0, 1.0, 1.0])) + (1.0,) for c in base_colors]\n",
    "    dark_colors = [tuple(0.92 * c[:3]) + (1.0,) for c in base_colors]\n",
    "\n",
    "    # Normalize zero-sums to small positive so pie draws uniformly\n",
    "    src_sizes = src.copy()\n",
    "    tgt_sizes = tgt.copy()\n",
    "    if src_sizes.sum() == 0:\n",
    "        src_sizes = _np.ones(B)\n",
    "    if tgt_sizes.sum() == 0:\n",
    "        tgt_sizes = _np.ones(B)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.axis('equal')\n",
    "\n",
    "    # Outer ring = source (lighter shades)\n",
    "    wedges_outer, _ = ax.pie(src_sizes, radius=1.3, colors=light_colors, startangle=90,\n",
    "                             wedgeprops=dict(width=0.4, edgecolor='white'))\n",
    "    # Inner ring = target (darker shades)\n",
    "    wedges_inner, _ = ax.pie(tgt_sizes, radius=0.9, colors=dark_colors, startangle=90,\n",
    "                             wedgeprops=dict(width=0.4, edgecolor='white'))\n",
    "\n",
    "    # Legend: colors map to band names (use dark color swatches), annotation explains outer/inner\n",
    "    legend_patches = [_Patch(facecolor=dark_colors[i], edgecolor='black', label=names[i]) for i in range(B)]\n",
    "    ax.legend(handles=legend_patches, bbox_to_anchor=(1.1, 0.8), title='Bands (dark=target)', fontsize=8)\n",
    "\n",
    "    ax.text(1.05, 0.4, 'Outer = source (lighter)\\nInner = target (darker)', transform=ax.transAxes,\n",
    "            fontsize=8, va='center', ha='left', bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "    plt.title('Summary counts: source (outer) and target (inner)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edbe5a",
   "metadata": {
    "id": "b7edbe5a"
   },
   "source": [
    "## Notes\n",
    "- This pipeline uses Gaussian TE (equivalent to Granger causality for Gaussian data) for speed and robustness.\n",
    "- Surrogates are FFT phase-randomized within each band to break directional structure while preserving power spectra.\n",
    "- Delay scan is done on broadband; band-wise TE then uses the chosen per-link delay.\n",
    "- Outputs per EDF: delay_scan_results.txt; per-link TE heatmaps, p-value maps, winning-band map, .npz and .txt summaries.\n",
    "- Customize bands via DEFAULT_BANDS at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf41c98",
   "metadata": {
    "id": "fcf41c98"
   },
   "source": [
    "## GPU and Parallel Enhancements\n",
    "\n",
    "- Optional GPU acceleration via CuPy/cuSignal for FFT-based surrogates (falls back to CPU automatically).\n",
    "- CPU speed-ups via numba JIT for Gaussian TE and joblib parallelization over links and band-pairs.\n",
    "- These overrides shadow earlier helpers; run this section after the main setup cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10de6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1757196511075,
     "user": {
      "displayName": "Francois",
      "userId": "00618482177956197410"
     },
     "user_tz": -60
    },
    "id": "6a10de6b",
    "outputId": "04f20134-fef6-4dab-c9f4-38db8d482c19"
   },
   "outputs": [],
   "source": [
    "# Try GPU (CuPy/cuSignal) and enable parallel + JIT\n",
    "import os, sys\n",
    "USE_GPU = True\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cupyx.scipy.signal as cpsignal\n",
    "    USE_GPU = True\n",
    "    print('CuPy/cuSignal available: GPU acceleration ON')\n",
    "except Exception:\n",
    "    print('CuPy/cuSignal not available: using CPU')\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "    NUMBA_ON = True\n",
    "except Exception:\n",
    "    NUMBA_ON = False\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Pure-Python fallback for TE (mirrors earlier gaussian_te)\n",
    "def _gaussian_te_py(x, y, delay_samples, k_past=1):\n",
    "    d = int(delay_samples)\n",
    "    if d <= 0:\n",
    "        return np.nan\n",
    "    T = min(len(x) - d - k_past, len(y) - k_past)\n",
    "    if T <= 5:\n",
    "        return np.nan\n",
    "    idx_y = np.arange(k_past, k_past + T)\n",
    "    idx_x = np.arange(k_past + d, k_past + d + T)\n",
    "    Yt = y[idx_y]\n",
    "    Ypast = np.column_stack([y[idx_y - i] for i in range(1, k_past + 1)])\n",
    "    Xpast = np.column_stack([x[idx_x - i] for i in range(1, k_past + 1)])\n",
    "    # LS residual variances\n",
    "    beta1, *_ = np.linalg.lstsq(Ypast, Yt, rcond=None)\n",
    "    r1 = Yt - Ypast @ beta1\n",
    "    n1, p1 = Ypast.shape\n",
    "    v1 = float(np.var(r1, ddof=min(p1, n1) - 1 if n1 > p1 else 1))\n",
    "    X2 = np.hstack([Ypast, Xpast])\n",
    "    beta2, *_ = np.linalg.lstsq(X2, Yt, rcond=None)\n",
    "    r2 = Yt - X2 @ beta2\n",
    "    n2, p2 = X2.shape\n",
    "    v2 = float(np.var(r2, ddof=min(p2, n2) - 1 if n2 > p2 else 1))\n",
    "    if v1 <= 0 or v2 <= 0:\n",
    "        return np.nan\n",
    "    return 0.5 * np.log(v1 / v2)\n",
    "\n",
    "if NUMBA_ON:\n",
    "    @njit(fastmath=True)\n",
    "    def _ls_res_var_jit(X, y):\n",
    "        # Ridge-regularized normal equations (Numba-friendly)\n",
    "        XT = X.T\n",
    "        XTX = XT @ X\n",
    "        eps = 1e-8\n",
    "        for i in range(XTX.shape[0]):\n",
    "            XTX[i, i] = XTX[i, i] + eps\n",
    "        XTy = XT @ y\n",
    "        beta = np.linalg.solve(XTX, XTy)\n",
    "        # residuals\n",
    "        resid = y - X @ beta\n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        denom = n - p\n",
    "        if denom < 1:\n",
    "            denom = n\n",
    "        rss = 0.0\n",
    "        for i in range(resid.shape[0]):\n",
    "            rss += resid[i] * resid[i]\n",
    "        return rss / denom\n",
    "\n",
    "    @njit(fastmath=True)\n",
    "    def _gaussian_te_jit(x, y, delay_samples, k_past):\n",
    "        d = int(delay_samples)\n",
    "        if d <= 0:\n",
    "            return np.nan\n",
    "        T = min(len(x) - d - k_past, len(y) - k_past)\n",
    "        if T <= 5:\n",
    "            return np.nan\n",
    "        Yt = np.empty(T, dtype=np.float64)\n",
    "        Ypast = np.empty((T, k_past), dtype=np.float64)\n",
    "        Xpast = np.empty((T, k_past), dtype=np.float64)\n",
    "        for t in range(T):\n",
    "            it_y = t + k_past\n",
    "            it_x = t + k_past + d\n",
    "            Yt[t] = y[it_y]\n",
    "            for i in range(k_past):\n",
    "                Ypast[t, i] = y[it_y - (i+1)]\n",
    "                Xpast[t, i] = x[it_x - (i+1)]\n",
    "        v1 = _ls_res_var_jit(Ypast, Yt)\n",
    "        X2 = np.empty((T, 2*k_past), dtype=np.float64)\n",
    "        for t in range(T):\n",
    "            for i in range(k_past):\n",
    "                X2[t, i] = Ypast[t, i]\n",
    "                X2[t, k_past + i] = Xpast[t, i]\n",
    "        v2 = _ls_res_var_jit(X2, Yt)\n",
    "        if v1 <= 0 or v2 <= 0:\n",
    "            return np.nan\n",
    "        return 0.5 * np.log(v1 / v2)\n",
    "\n",
    "    def gaussian_te(x, y, delay_samples, k_past=1):\n",
    "        # Try JIT; fall back to Python on any failure (e.g., worker compile issues)\n",
    "        try:\n",
    "            return _gaussian_te_jit(np.asarray(x, np.float64), np.asarray(y, np.float64), int(delay_samples), int(k_past))\n",
    "        except Exception:\n",
    "            return _gaussian_te_py(np.asarray(x, np.float64), np.asarray(y, np.float64), int(delay_samples), int(k_past))\n",
    "\n",
    "# GPU-aware FFT phase randomization (batched)\n",
    "def phase_randomize_batch(X, rng, use_gpu=USE_GPU):\n",
    "    X = np.asarray(X, np.float64)\n",
    "    if X.size == 0:\n",
    "        return np.empty_like(X)  # Return empty array if input is empty\n",
    "    N, T = X.shape\n",
    "    if use_gpu:\n",
    "        xg = cp.asarray(X)\n",
    "        Xf = cp.fft.rfft(xg, axis=1)\n",
    "        mags = cp.abs(Xf)\n",
    "        phs = cp.angle(Xf)\n",
    "        rand = rng.uniform(-np.pi, np.pi, size=phs.shape)\n",
    "        R = cp.asarray(rand)\n",
    "        R[:, 0] = 0.0\n",
    "        if T % 2 == 0:\n",
    "            R[:, -1] = 0.0\n",
    "        Yf = mags * cp.exp(1j * (phs + R))\n",
    "        Y = cp.fft.irfft(Yf, n=T, axis=1).get()\n",
    "        return Y\n",
    "    else:\n",
    "        Xf = np.fft.rfft(X, axis=1)\n",
    "        mags = np.abs(Xf)\n",
    "        phs = np.angle(Xf)\n",
    "        R = rng.uniform(-np.pi, np.pi, size=phs.shape)\n",
    "        R[:, 0] = 0.0\n",
    "        if T % 2 == 0:\n",
    "            R[:, -1] = 0.0\n",
    "        Yf = mags * np.exp(1j * (phs + R))\n",
    "        Y = np.fft.irfft(Yf, n=T, axis=1)\n",
    "        return Y\n",
    "\n",
    "# Override compute_te_grid_with_surrogates with parallel + batched surrogates\n",
    "def compute_te_grid_with_surrogates(src, tgt, fs, bands, delay_samples, k_past=1, n_surr=100, alpha=0.05, downsample=True, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(123)\n",
    "    names = list(bands.keys())\n",
    "    B = len(names)\n",
    "    TE = np.full((B, B), np.nan, float)\n",
    "    pS = np.full((B, B), np.nan, float)\n",
    "    pT = np.full((B, B), np.nan, float)\n",
    "\n",
    "    # Pre-filter cache\n",
    "    filt_src, fs_src = {}, {}\n",
    "    filt_tgt, fs_tgt = {}, {}\n",
    "    for b in names:\n",
    "        filt_src[b], fs_src[b] = bandpass(src, fs, bands[b], order=4, decimate=downsample)\n",
    "        filt_tgt[b], fs_tgt[b] = bandpass(tgt, fs, bands[b], order=4, decimate=downsample)\n",
    "\n",
    "    def compute_cell(i, j):\n",
    "        xs = filt_src[names[i]]; fs_x = fs_src[names[i]]\n",
    "        yt = filt_tgt[names[j]]; fs_y = fs_tgt[names[j]]\n",
    "        fs_c = min(fs_x, fs_y)\n",
    "        if abs(fs_x - fs_c) > 1e-6:\n",
    "            xs = sp.signal.resample_poly(xs, up=int(fs_c), down=int(fs_x))\n",
    "        if abs(fs_y - fs_c) > 1e-6:\n",
    "            yt = sp.signal.resample_poly(yt, up=int(fs_c), down=int(fs_y))\n",
    "        d_samp = max(1, int(round(delay_samples * (fs_c / fs))))\n",
    "        v_obs = gaussian_te(xs, yt, d_samp, k_past=k_past)\n",
    "        if not np.isfinite(v_obs):\n",
    "            return (i, j, np.nan, np.nan, np.nan)\n",
    "        # Batched surrogates (source-only and target-only)\n",
    "        batch = max(1, n_surr // 2)\n",
    "        XS = np.tile(xs, (batch, 1))\n",
    "        YT = np.tile(yt, (batch, 1))\n",
    "        XS_s = phase_randomize_batch(XS, rng, use_gpu=USE_GPU)\n",
    "        PS = np.empty(batch, float)\n",
    "        for k in range(batch):\n",
    "            PS[k] = gaussian_te(XS_s[k], yt, d_samp, k_past=k_past)\n",
    "        p_src = float(np.nanmean(PS >= v_obs))\n",
    "        YT_s = phase_randomize_batch(YT, rng, use_gpu=USE_GPU)\n",
    "        PT = np.empty(batch, float)\n",
    "        for k in range(batch):\n",
    "            PT[k] = gaussian_te(xs, YT_s[k], d_samp, k_past=k_past)\n",
    "        p_tgt = float(np.nanmean(PT >= v_obs))\n",
    "        return (i, j, v_obs, p_src, p_tgt)\n",
    "\n",
    "    # Fill grid in parallel (use threads to avoid Numba compile issues in subprocesses)\n",
    "    results = Parallel(n_jobs=N_JOBS, prefer=\"threads\")(delayed(compute_cell)(i, j) for i in range(B) for j in range(B))\n",
    "    for (i, j, v, ps, pt) in results:\n",
    "        TE[i, j] = v\n",
    "        pS[i, j] = ps\n",
    "        pT[i, j] = pt\n",
    "\n",
    "    # FDR per grid\n",
    "    sigS, critS = fdr_bh(pS, alpha=alpha)\n",
    "    sigT, critT = fdr_bh(pT, alpha=alpha)\n",
    "    return names, TE, pS, pT, sigS, sigT, float(critS), float(critT)\n",
    "\n",
    "print('GPU/Parallel overrides active. Set N_JOBS at top to control CPU parallelism.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144664d",
   "metadata": {},
   "source": [
    "## Main Script Launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e3678",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 81043,
     "status": "error",
     "timestamp": 1757196595096,
     "user": {
      "displayName": "Francois",
      "userId": "00618482177956197410"
     },
     "user_tz": -60
    },
    "id": "3a2e3678",
    "outputId": "f9b49b8d-d186-4392-ac4b-471e117dd51e"
   },
   "outputs": [],
   "source": [
    "# Main entry: simple launcher for interactive processing\n",
    "import sys, traceback\n",
    "print(\"Transfer Entropy – Main Menu\")\n",
    "print(\"1) Run (you'll select single-file or folder next)\\nQ) Quit\")\n",
    "choice = input(\"Choose an option [1/Q]: \").strip().lower()\n",
    "if choice in (\"1\", \"\", \"r\", \"run\"):\n",
    "    rb = globals().get(\"run_batch\")\n",
    "    if rb is None or not callable(rb):\n",
    "        print(\"run_batch is not defined. Please run the EDF/FIF runner cell above (the one that defines process_one_file and run_batch).\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"Using run_batch defined at: {rb.__code__.co_filename}, line {rb.__code__.co_firstlineno}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        rb()\n",
    "else:\n",
    "    print(\"Exit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffcf86c",
   "metadata": {},
   "source": [
    "## Summary Visualisations Launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8880646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Build 5% and 1% (alpha=0.05 and 0.01) summary visualizations from existing per-link .npz files\n",
    "# Modes: 1) Single folder, 2) Batch over subfolders, 3) Meta-analysis across all subfolders\n",
    "\n",
    "def process_single_folder(out_dir):\n",
    "    # [Existing code for single folder processing]\n",
    "    import glob, csv\n",
    "    if not os.path.isdir(out_dir):\n",
    "        print('Folder not found:', out_dir)\n",
    "        return\n",
    "    npz_files = sorted(glob.glob(os.path.join(out_dir, 'link_*.npz')))\n",
    "    if len(npz_files) == 0:\n",
    "        npz_files = sorted(glob.glob(os.path.join(out_dir, '*_results.npz')))\n",
    "    if len(npz_files) == 0:\n",
    "        print('No per-link .npz files found in', out_dir)\n",
    "        return\n",
    "    base = os.path.basename(out_dir.rstrip(os.sep))\n",
    "    patient_name = base.split('_')[0]\n",
    "    alphas = [0.05, 0.01]\n",
    "    summaries = {}\n",
    "    for alpha in alphas:\n",
    "        band_names = None\n",
    "        total_src_counts = None\n",
    "        total_tgt_counts = None\n",
    "        all_pS = []\n",
    "        all_pT = []\n",
    "        for fpath in npz_files:\n",
    "            try:\n",
    "                arr = np.load(fpath, allow_pickle=True)\n",
    "            except Exception as e:\n",
    "                print('Skipping', fpath, '->', e); continue\n",
    "            try:\n",
    "                names_raw = arr['names']\n",
    "            except Exception:\n",
    "                print('Skipping', fpath, ': no names field'); continue\n",
    "            names = [n.decode('utf-8') if isinstance(n, (bytes, bytearray)) else str(n) for n in np.asarray(names_raw).ravel()]\n",
    "            try:\n",
    "                pS = arr['p_source']\n",
    "                pT = arr['p_target']\n",
    "            except Exception:\n",
    "                print('Skipping', fpath, ': missing p_source/p_target'); continue\n",
    "            if band_names is None:\n",
    "                band_names = names\n",
    "                B = len(band_names)\n",
    "                total_src_counts = np.zeros(B, dtype=int)\n",
    "                total_tgt_counts = np.zeros(B, dtype=int)\n",
    "            if pS.shape != (B, B) or pT.shape != (B, B):\n",
    "                print('Skipping', fpath, ': shape mismatch pS', getattr(pS, 'shape', None))\n",
    "                continue\n",
    "            signif = (pS <= alpha) & (pT <= alpha)\n",
    "            total_src_counts += np.sum(signif, axis=1)\n",
    "            total_tgt_counts += np.sum(signif, axis=0)\n",
    "            all_pS.append(pS)\n",
    "            all_pT.append(pT)\n",
    "        if band_names is None:\n",
    "            print(f'No usable .npz files found for alpha={alpha}.')\n",
    "            continue\n",
    "        summaries[alpha] = {\n",
    "            'band_names': band_names,\n",
    "            'total_src_counts': total_src_counts,\n",
    "            'total_tgt_counts': total_tgt_counts,\n",
    "            'all_pS': all_pS,\n",
    "            'all_pT': all_pT\n",
    "        }\n",
    "    if not summaries:\n",
    "        print('No summaries could be built.')\n",
    "        return\n",
    "    # Create figure with 2x2 subplots: top row pies, bottom row stacked bars for top3 sources\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    for idx, alpha in enumerate(alphas):\n",
    "        if alpha not in summaries:\n",
    "            continue\n",
    "        data = summaries[alpha]\n",
    "        band_names = data['band_names']\n",
    "        total_src_counts = data['total_src_counts']\n",
    "        total_tgt_counts = data['total_tgt_counts']\n",
    "        all_pS = data['all_pS']\n",
    "        all_pT = data['all_pT']\n",
    "        # Pie subplot (top row)\n",
    "        pie_ax = axes[0, idx]\n",
    "        pie_ax.axis('equal')\n",
    "        B = len(band_names)\n",
    "        src = np.asarray(total_src_counts, dtype=float)\n",
    "        tgt = np.asarray(total_tgt_counts, dtype=float)\n",
    "        if src.sum() == 0 and tgt.sum() == 0:\n",
    "            pie_ax.text(0.5, 0.5, 'No significant source-target pairs found', ha='center', va='center', transform=pie_ax.transAxes)\n",
    "            continue\n",
    "        base_colors = plt.get_cmap(CMAP_BAND)(np.linspace(0, 1, B))\n",
    "        light_colors = [tuple(0.65 * c[:3] + 0.35 * np.array([1.0, 1.0, 1.0])) + (1.0,) for c in base_colors]\n",
    "        dark_colors = [tuple(0.92 * c[:3]) + (1.0,) for c in base_colors]\n",
    "        src_sizes = src.copy()\n",
    "        tgt_sizes = tgt.copy()\n",
    "        if src_sizes.sum() == 0:\n",
    "            src_sizes = np.ones(B)\n",
    "        if tgt_sizes.sum() == 0:\n",
    "            tgt_sizes = np.ones(B)\n",
    "        # Two rings: inner (targets, dark), outer (sources, light)\n",
    "        wedges_inner = pie_ax.pie(tgt_sizes, radius=0.9, colors=dark_colors, startangle=90, wedgeprops=dict(width=0.4, edgecolor='white'), autopct='')[0]\n",
    "        wedges_outer = pie_ax.pie(src_sizes, radius=1.3, colors=light_colors, startangle=90, wedgeprops=dict(width=0.4, edgecolor='white'), autopct='')[0]\n",
    "        # Add percentage labels with lines for outer, inside for inner\n",
    "        for wedge in wedges_inner:\n",
    "            theta = (wedge.theta1 + wedge.theta2) / 2\n",
    "            r = 0.45  # inside inner ring\n",
    "            x = r * np.cos(np.deg2rad(theta))\n",
    "            y = r * np.sin(np.deg2rad(theta))\n",
    "            pct = (wedge.theta2 - wedge.theta1) / 360 * 100\n",
    "            if pct > 1:  # only label if >1%\n",
    "                pie_ax.text(x, y, f'{pct:.1f}%', ha='center', va='center', fontsize=8, color='black')\n",
    "        for wedge in wedges_outer:\n",
    "            theta = (wedge.theta1 + wedge.theta2) / 2\n",
    "            r_mid = 1.1  # midpoint of outer ring\n",
    "            x_mid = r_mid * np.cos(np.deg2rad(theta))\n",
    "            y_mid = r_mid * np.sin(np.deg2rad(theta))\n",
    "            r_label = 1.5  # outside\n",
    "            x_label = r_label * np.cos(np.deg2rad(theta))\n",
    "            y_label = r_label * np.sin(np.deg2rad(theta))\n",
    "            pct = (wedge.theta2 - wedge.theta1) / 360 * 100\n",
    "            if pct > 1:  # only label if >1%\n",
    "                pie_ax.annotate(f'{pct:.1f}%', xy=(x_mid, y_mid), xytext=(x_label, y_label), \n",
    "                                arrowprops=dict(arrowstyle='-', color='black', lw=0.5, shrinkA=0, shrinkB=0), \n",
    "                                ha='center', va='center', fontsize=8)\n",
    "        # Legend for pie\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_patches = [Patch(facecolor=dark_colors[i], edgecolor='black', label=band_names[i]) for i in range(B)]\n",
    "        pie_ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 0.8), title='Bands', fontsize=8)\n",
    "        pie_ax.set_title(f'Alpha={alpha*100:.0f}%: Inner=Targets (dark), Outer=Sources (light)')\n",
    "        # Bar subplot (bottom row) - horizontal stacked bars for top3 sources' target distributions\n",
    "        bar_ax = axes[1, idx]\n",
    "        top3_indices = np.argsort(total_src_counts)[-3:][::-1]  # descending\n",
    "        y_positions = np.arange(len(top3_indices))\n",
    "        for bar_idx, i in enumerate(top3_indices):\n",
    "            tgt_dist = np.zeros(B)\n",
    "            for link_idx, (ps, pt) in enumerate(zip(all_pS, all_pT)):\n",
    "                signif = (ps[i, :] <= alpha) & (pt[i, :] <= alpha)\n",
    "                tgt_dist += signif.astype(int)\n",
    "            if tgt_dist.sum() == 0:\n",
    "                tgt_dist = np.ones(B)  # uniform if no data\n",
    "            # Normalize to sum to 1 for bar\n",
    "            tgt_dist = tgt_dist / tgt_dist.sum()\n",
    "            left = 0\n",
    "            for j in range(B):\n",
    "                bar_ax.barh(y_positions[bar_idx], tgt_dist[j], left=left, color=dark_colors[j], edgecolor='white', height=0.8)\n",
    "                left += tgt_dist[j]\n",
    "        bar_ax.set_yticks(y_positions)\n",
    "        bar_ax.set_yticklabels([band_names[i] for i in top3_indices])\n",
    "        bar_ax.set_xlabel('Proportion of Target Bands')\n",
    "        bar_ax.set_title(f'Top 3 Sources\\' Target Distributions (Alpha={alpha*100:.0f}%)')\n",
    "        bar_ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 0.8), title='Target Bands', fontsize=8)\n",
    "    fig.suptitle(f'Drivers of communication between NIZ and SOZ for {patient_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    png_path = os.path.join(out_dir, f'{base}_enhanced_summary.png')\n",
    "    plt.savefig(png_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print('Saved enhanced summary figure:', png_path)\n",
    "    # Save TXT with percentages table\n",
    "    txt_summary_path = os.path.join(out_dir, f'{base}_enhanced_summary.txt')\n",
    "    with open(txt_summary_path, 'w') as f:\n",
    "        f.write(f'# Enhanced Summary for {patient_name}\\n')\n",
    "        f.write('# Percentages from Pie and Bar Charts\\n\\n')\n",
    "        for idx, alpha in enumerate(alphas):\n",
    "            if alpha not in summaries:\n",
    "                continue\n",
    "            data = summaries[alpha]\n",
    "            band_names = data['band_names']\n",
    "            total_src_counts = data['total_src_counts']\n",
    "            total_tgt_counts = data['total_tgt_counts']\n",
    "            all_pS = data['all_pS']\n",
    "            all_pT = data['all_pT']\n",
    "            f.write(f'# Alpha {alpha*100:.0f}%\\n')\n",
    "            f.write('# Pie Chart Percentages\\n')\n",
    "            f.write('Band\\tInner (Targets %)\\tOuter (Sources %)\\n')\n",
    "            src_total = total_src_counts.sum()\n",
    "            tgt_total = total_tgt_counts.sum()\n",
    "            for nm, s, t in zip(band_names, total_src_counts, total_tgt_counts):\n",
    "                src_pct = (s / src_total * 100) if src_total > 0 else 0\n",
    "                tgt_pct = (t / tgt_total * 100) if tgt_total > 0 else 0\n",
    "                f.write(f'{nm}\\t{src_pct:.1f}\\t{tgt_pct:.1f}\\n')\n",
    "            f.write('\\n')\n",
    "            f.write('# Bar Chart: Top 3 Sources\\' Target Distributions\\n')\n",
    "            f.write('Source Band\\tTarget Band\\tProportion\\n')\n",
    "            top3_indices = np.argsort(total_src_counts)[-3:][::-1]\n",
    "            for i in top3_indices:\n",
    "                tgt_dist = np.zeros(len(band_names))\n",
    "                for link_idx, (ps, pt) in enumerate(zip(all_pS, all_pT)):\n",
    "                    signif = (ps[i, :] <= alpha) & (pt[i, :] <= alpha)\n",
    "                    tgt_dist += signif.astype(int)\n",
    "                if tgt_dist.sum() == 0:\n",
    "                    tgt_dist = np.ones(len(band_names))\n",
    "                tgt_dist = tgt_dist / tgt_dist.sum()\n",
    "                for j, nm_tgt in enumerate(band_names):\n",
    "                    f.write(f'{band_names[i]}\\t{nm_tgt}\\t{tgt_dist[j]:.3f}\\n')\n",
    "            f.write('\\n')\n",
    "    print('Saved enhanced summary TXT:', txt_summary_path)\n",
    "    # Also save CSVs and TXTs for each alpha\n",
    "    for alpha in alphas:\n",
    "        if alpha in summaries:\n",
    "            data = summaries[alpha]\n",
    "            base_out_file = os.path.join(out_dir, f'{base}_alpha{int(alpha*100):02d}')\n",
    "            csv_path = base_out_file + '_band_counts.csv'\n",
    "            with open(csv_path, 'w', newline='') as cf:\n",
    "                writer = csv.writer(cf)\n",
    "                writer.writerow(['band', 'total_src_count', 'total_tgt_count'])\n",
    "                for nm, s, t in zip(data['band_names'], data['total_src_counts'], data['total_tgt_counts']):\n",
    "                    writer.writerow([nm, int(s), int(t)])\n",
    "            txt_path = base_out_file + '_summary.txt'\n",
    "            with open(txt_path, 'w') as fh:\n",
    "                fh.write(f'# Aggregated significant band counts at alpha={alpha}\\n')\n",
    "                fh.write('# format: band,total_source_count,total_target_count\\n')\n",
    "                for nm, s, t in zip(data['band_names'], data['total_src_counts'], data['total_tgt_counts']):\n",
    "                    fh.write(f\"{nm},{s},{t}\\n\")\n",
    "            print(f'Saved alpha={alpha*100:.0f}% files: CSV={csv_path}, TXT={txt_path}')\n",
    "\n",
    "\n",
    "def meta_analyze(parent_dir):\n",
    "    import glob\n",
    "    txt_files = []\n",
    "    for sub in os.listdir(parent_dir):\n",
    "        sub_path = os.path.join(parent_dir, sub)\n",
    "        if os.path.isdir(sub_path):\n",
    "            txt_file = os.path.join(sub_path, f'{sub}_enhanced_summary.txt')\n",
    "            if os.path.isfile(txt_file):\n",
    "                txt_files.append((sub, txt_file))\n",
    "    if not txt_files:\n",
    "        print('No enhanced summary TXT files found.')\n",
    "        return\n",
    "    # Aggregate data\n",
    "    all_src_pcts = {}\n",
    "    all_tgt_pcts = {}\n",
    "    all_src_dists = {}\n",
    "    band_names = None\n",
    "    for patient, txt_file in txt_files:\n",
    "        with open(txt_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        current_alpha = None\n",
    "        section = None\n",
    "        pie_header_cols = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('# Alpha'):\n",
    "                try:\n",
    "                    tok = line.split()[2]\n",
    "                    current_alpha = float(tok.rstrip('%')) / 100\n",
    "                except Exception:\n",
    "                    current_alpha = None\n",
    "                section = None\n",
    "                pie_header_cols = None\n",
    "                continue\n",
    "            if line.startswith('# Pie Chart Percentages'):\n",
    "                section = 'pie'\n",
    "                continue\n",
    "            if line.startswith('Band\\t') and section == 'pie':\n",
    "                pie_header_cols = line.split('\\t')\n",
    "                continue\n",
    "            if line.startswith('# Bar Chart'):\n",
    "                section = 'bar'\n",
    "                continue\n",
    "            if line.startswith('Source Band\\t') and section == 'bar':\n",
    "                continue\n",
    "            # Parse pie rows for alpha=0.01\n",
    "            if '\\t' in line and current_alpha == 0.01 and section == 'pie':\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    band = parts[0]\n",
    "                    # determine order using header when possible\n",
    "                    if pie_header_cols and len(pie_header_cols) == 3:\n",
    "                        col1 = pie_header_cols[1].lower()\n",
    "                        col2 = pie_header_cols[2].lower()\n",
    "                        if 'target' in col1 and 'source' in col2:\n",
    "                            tgt_pct_s = parts[1]\n",
    "                            src_pct_s = parts[2]\n",
    "                        elif 'source' in col1 and 'target' in col2:\n",
    "                            src_pct_s = parts[1]\n",
    "                            tgt_pct_s = parts[2]\n",
    "                        else:\n",
    "                            # fallback to writer-convention: second=source, third=target\n",
    "                            src_pct_s = parts[1]\n",
    "                            tgt_pct_s = parts[2]\n",
    "                    else:\n",
    "                        src_pct_s = parts[1]\n",
    "                        tgt_pct_s = parts[2]\n",
    "                    try:\n",
    "                        src_pct = float(str(src_pct_s).rstrip('%'))\n",
    "                        tgt_pct = float(str(tgt_pct_s).rstrip('%'))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if band_names is None:\n",
    "                        band_names = []\n",
    "                    if band not in band_names:\n",
    "                        band_names.append(band)\n",
    "                    all_src_pcts.setdefault(band, []).append(float(src_pct))\n",
    "                    all_tgt_pcts.setdefault(band, []).append(float(tgt_pct))\n",
    "                continue\n",
    "            # Parse bar rows for alpha=0.01\n",
    "            if '\\t' in line and len(line.split('\\t')) == 3 and current_alpha == 0.01 and section == 'bar':\n",
    "                src, tgt, prop_s = line.split('\\t')\n",
    "                try:\n",
    "                    prop = float(prop_s)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if src not in all_src_dists:\n",
    "                    all_src_dists[src] = {}\n",
    "                if tgt not in all_src_dists[src]:\n",
    "                    all_src_dists[src][tgt] = []\n",
    "                all_src_dists[src][tgt].append(prop)\n",
    "    if not band_names:\n",
    "        print('No data to aggregate.')\n",
    "        return\n",
    "    # Ensure numeric lists (coerce) and compute medians\n",
    "    for b in band_names:\n",
    "        all_src_pcts[b] = [float(x) for x in all_src_pcts.get(b, [])]\n",
    "        all_tgt_pcts[b] = [float(x) for x in all_tgt_pcts.get(b, [])]\n",
    "    src_medians = {b: float(np.median(all_src_pcts.get(b, [0.0]))) for b in band_names}\n",
    "    tgt_medians = {b: float(np.median(all_tgt_pcts.get(b, [0.0]))) for b in band_names}\n",
    "    # Top 3 sources\n",
    "    top3_src = sorted(src_medians, key=src_medians.get, reverse=True)[:3]\n",
    "    # For each top source, top target\n",
    "    top_targets = {}\n",
    "    for src in top3_src:\n",
    "        if src in all_src_dists:\n",
    "            tgt_props = {tgt: float(np.median(all_src_dists[src].get(tgt, [0.0]))) for tgt in band_names}\n",
    "            top_targets[src] = max(tgt_props, key=tgt_props.get)\n",
    "    # Statistical test: Kruskal-Wallis for differences in src pcts across all bands\n",
    "    from scipy.stats import kruskal\n",
    "    groups = [all_src_pcts.get(b, []) for b in band_names if all_src_pcts.get(b)]\n",
    "    if len(groups) > 1:\n",
    "        try:\n",
    "            stat, p = kruskal(*groups)\n",
    "            test_name = f'Kruskal-Wallis H={stat:.2f}, p={p:.3f}'\n",
    "        except Exception:\n",
    "            test_name = 'Kruskal-Wallis failed'\n",
    "            p = 1.0\n",
    "    else:\n",
    "        test_name = 'No test (insufficient data)'\n",
    "        p = 1.0\n",
    "    # Traditional visualizations: nested pie and horizontal stacked bars (horizontal layout)\n",
    "    fig, (pie_ax, bar_ax) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    B = len(band_names)\n",
    "    # Nested pie chart (left)\n",
    "    pie_ax.axis('equal')\n",
    "    src_vals = np.array([src_medians[b] for b in band_names])\n",
    "    tgt_vals = np.array([tgt_medians[b] for b in band_names])\n",
    "    if src_vals.sum() == 0:\n",
    "        src_vals = np.ones(B)\n",
    "    if tgt_vals.sum() == 0:\n",
    "        tgt_vals = np.ones(B)\n",
    "    # Use vibrant colormap\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    base_colors = cmap(np.linspace(0, 1, B))\n",
    "    light_colors = [tuple(0.6 * c[:3] + 0.4 * np.array([1.0, 1.0, 1.0])) + (1.0,) for c in base_colors]\n",
    "    dark_colors = [tuple(0.8 * c[:3]) + (1.0,) for c in base_colors]\n",
    "    # Inner pie (targets, dark)\n",
    "    wedges_inner = pie_ax.pie(tgt_vals, radius=0.7, colors=dark_colors, startangle=90, wedgeprops=dict(width=0.3, edgecolor='black', linewidth=1), autopct=lambda pct: f'{pct:.1f}%' if pct > 1 else '', pctdistance=0.8, textprops={'fontsize': 10, 'color': 'white'})\n",
    "    # Outer pie (sources, light)\n",
    "    wedges_outer = pie_ax.pie(src_vals, radius=1.0, colors=light_colors, startangle=90, wedgeprops=dict(width=0.3, edgecolor='black', linewidth=1), autopct=lambda pct: f'{pct:.1f}%' if pct > 1 else '', pctdistance=0.85, textprops={'fontsize': 10, 'color': 'black'})\n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_patches = [Patch(facecolor=dark_colors[i], edgecolor='black', label=f'{band_names[i]} (Target)') for i in range(B)] + [Patch(facecolor=light_colors[i], edgecolor='black', label=f'{band_names[i]} (Source)') for i in range(B)]\n",
    "    pie_ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9, title='Bands')\n",
    "    pie_ax.set_title(f'Median Source & Target % (Nested Pie)\\np = {p:.3f}', fontsize=14, fontweight='bold')\n",
    "    # Horizontal stacked bars (right)\n",
    "    y_positions = np.arange(len(top3_src))\n",
    "    bar_ax.set_yticks(y_positions)\n",
    "    bar_ax.set_yticklabels(top3_src)\n",
    "    bar_ax.set_xlabel('Proportion of Target Bands', fontsize=12)\n",
    "    bar_ax.set_title('Top 3 Sources\\' Target Distributions', fontsize=14, fontweight='bold')\n",
    "    for i, src in enumerate(top3_src):\n",
    "        if src in all_src_dists:\n",
    "            tgt_props = {tgt: float(np.median(all_src_dists[src].get(tgt, [0.0]))) for tgt in band_names}\n",
    "            props = [tgt_props.get(b, 0.0) for b in band_names]\n",
    "            if sum(props) == 0:\n",
    "                props = [1.0 / B] * B\n",
    "            left = 0\n",
    "            for j, prop in enumerate(props):\n",
    "                bar_ax.barh(i, prop, left=left, color=dark_colors[j], edgecolor='black', height=0.6)\n",
    "                left += prop\n",
    "    # Add asterisks for top targets\n",
    "    for i, src in enumerate(top3_src):\n",
    "        top_tgt = top_targets.get(src)\n",
    "        if top_tgt:\n",
    "            tgt_idx = band_names.index(top_tgt)\n",
    "            bar_ax.text(left + 0.02, i, '*', ha='left', va='center', fontsize=16, color='red', fontweight='bold')\n",
    "    bar_ax.legend(handles=[Patch(facecolor=dark_colors[j], edgecolor='black', label=band_names[j]) for j in range(B)] + [plt.Line2D([0], [0], marker='*', color='red', markersize=10, linestyle='None', label='Top Target (p < 0.01)')], bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9, title='Target Bands')\n",
    "    # Overall title and test info\n",
    "    fig.suptitle(f'Meta-Analysis: Transfer Entropy Across Patients\\n{test_name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    meta_png = os.path.join(parent_dir, 'meta_analysis_traditional.png')\n",
    "    plt.savefig(meta_png, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print('Saved meta-analysis traditional figure:', meta_png)\n",
    "    # Second figure: Summary table\n",
    "    fig2, ax2 = plt.subplots(figsize=(12, 6))\n",
    "    ax2.axis('off')\n",
    "    # Calculate summary data\n",
    "    total_links = sum(len(all_src_pcts.get(b, [])) for b in band_names)\n",
    "    signif_links = sum(1 for b in band_names for pct in all_src_pcts.get(b, []) if pct > 0)\n",
    "    table_data = [\n",
    "        ['Metric', 'Value'],\n",
    "        ['Total Patients', str(len(txt_files))],\n",
    "        ['Total Links', str(total_links)],\n",
    "        ['Significant Links', str(signif_links)],\n",
    "        ['Top Sources', ', '.join(top3_src)],\n",
    "        ['Top Targets', ', '.join([top_targets.get(s, 'N/A') for s in top3_src])],\n",
    "        ['Kruskal-Wallis p-value', f'{p:.3f}'],\n",
    "        ['Significant? (p < 0.01)', 'Yes' if p < 0.01 else 'No']\n",
    "    ]\n",
    "    table = ax2.table(cellText=table_data, colLabels=None, cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "    ax2.set_title('Meta-Analysis Summary Table', fontsize=14, fontweight='bold')\n",
    "    meta_table_png = os.path.join(parent_dir, 'meta_analysis_summary_table.png')\n",
    "    plt.savefig(meta_table_png, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print('Saved meta-analysis summary table:', meta_table_png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visible Summary Visualisations Launcher (compact)\n",
    "print('Summary Visualisations Launcher')\n",
    "print('Choose mode: (1) Single folder, (2) Batch subfolders, (3) Meta-analysis, Q to quit')\n",
    "mode = input('Mode [1/2/3/Q]: ').strip().lower()\n",
    "if mode in ('q', 'quit'):\n",
    "    print('Cancelled.')\n",
    "else:\n",
    "    try:\n",
    "        if mode == '1' or mode == 'single':\n",
    "            out_dir = input('Enter per-EDF output folder (full path): ').strip()\n",
    "            if not out_dir:\n",
    "                print('No folder provided.')\n",
    "            else:\n",
    "                process_single_folder(out_dir)\n",
    "        elif mode == '2' or mode == 'batch':\n",
    "            parent_dir = input('Enter parent directory containing output folders: ').strip()\n",
    "            if not parent_dir or not os.path.isdir(parent_dir):\n",
    "                print('Invalid directory.')\n",
    "            else:\n",
    "                for sub in sorted(os.listdir(parent_dir)):\n",
    "                    sub_path = os.path.join(parent_dir, sub)\n",
    "                    if os.path.isdir(sub_path):\n",
    "                        print(f'Processing {sub}...')\n",
    "                        try:\n",
    "                            process_single_folder(sub_path)\n",
    "                        except Exception as e:\n",
    "                            print(f'Error processing {sub}: {e}')\n",
    "        elif mode == '3' or mode == 'meta':\n",
    "            parent_dir = input('Enter parent directory for meta-analysis: ').strip()\n",
    "            if not parent_dir or not os.path.isdir(parent_dir):\n",
    "                print('Invalid directory.')\n",
    "            else:\n",
    "                meta_analyze(parent_dir)\n",
    "        else:\n",
    "            print('Invalid mode.')\n",
    "    except NameError as ne:\n",
    "        print('Required function not found in the notebook. Make sure you ran the cell that defines process_single_folder and meta_analyze.')\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print('Launcher failed with:', e)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
